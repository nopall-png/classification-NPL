{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 1. SETUP & PREPARASI DATA\n",
        "# =========================================================\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, GlobalMaxPool1D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Ekstrak Dataset\n",
        "with zipfile.ZipFile('archive.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('dataset_news')\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_json('dataset_news/News_Category_Dataset_v3.json', lines=True)\n",
        "\n",
        "# Mapping 4 Topic Utama\n",
        "category_map = {\n",
        "    'BUSINESS': 'economy', 'MONEY': 'economy',\n",
        "    'ENTERTAINMENT': 'entertainment', 'COMEDY': 'entertainment',\n",
        "    'WELLNESS': 'health', 'HEALTHY LIVING': 'health',\n",
        "    'TECH': 'technology', 'SCIENCE': 'technology'\n",
        "}\n",
        "df['target_topic'] = df['category'].map(category_map)\n",
        "df = df.dropna(subset=['target_topic']).reset_index(drop=True)\n",
        "\n",
        "# Ambil 20.000 data agar stabil\n",
        "df = df.sample(20000, random_state=42)"
      ],
      "metadata": {
        "id": "yYzPTu6OCJKh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2. MODEL 1: LSTM CLASSIFIER\n",
        "# =========================================================\n",
        "print(\"--- Memulai Training Model 1: LSTM Classifier ---\")\n",
        "\n",
        "df['combined_text'] = df['headline'].astype(str) + \" \" + df['short_description'].astype(str)\n",
        "X_class = df['combined_text']\n",
        "y_class = pd.get_dummies(df['target_topic'])\n",
        "categories = y_class.columns.tolist()\n",
        "\n",
        "num_words_class = 5000\n",
        "class_tokenizer = Tokenizer(num_words=num_words_class)\n",
        "class_tokenizer.fit_on_texts(X_class)\n",
        "X_class_pad = pad_sequences(class_tokenizer.texts_to_sequences(X_class), maxlen=70, padding='post')\n",
        "\n",
        "# Arsitektur Sederhana agar akurasi tidak \"over-performer\"\n",
        "class_input = Input(shape=(70,))\n",
        "x_cl = Embedding(input_dim=num_words_class, output_dim=64)(class_input)\n",
        "x_cl = LSTM(64, return_sequences=True)(x_cl)\n",
        "x_cl = GlobalMaxPool1D()(x_cl)\n",
        "x_cl = Dropout(0.5)(x_cl)\n",
        "class_output = Dense(len(categories), activation='softmax')(x_cl)\n",
        "\n",
        "model_class = Model(class_input, class_output)\n",
        "model_class.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Mengubah epochs menjadi 5\n",
        "model_class.fit(X_class_pad, y_class, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Simpan Model Klasifikasi\n",
        "model_class.save('lstm_genre_classifier.h5')\n",
        "with open('class_tokenizer.pkl', 'wb') as f: pickle.dump(class_tokenizer, f)\n",
        "with open('categories_label.pkl', 'wb') as f: pickle.dump(categories, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dE71nvNCKDx",
        "outputId": "4bd0e695-fdcf-4e00-abf7-5afa00adc080"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Memulai Training Model 1: LSTM Classifier ---\n",
            "Epoch 1/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.5471 - loss: 1.0847 - val_accuracy: 0.7305 - val_loss: 0.6926\n",
            "Epoch 2/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.7678 - loss: 0.6168 - val_accuracy: 0.8070 - val_loss: 0.5560\n",
            "Epoch 3/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8457 - loss: 0.4398 - val_accuracy: 0.8140 - val_loss: 0.5337\n",
            "Epoch 4/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8867 - loss: 0.3406 - val_accuracy: 0.8295 - val_loss: 0.5854\n",
            "Epoch 5/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9180 - loss: 0.2691 - val_accuracy: 0.8260 - val_loss: 0.5551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 3. MODEL 2: LSTM SUMMARIZER\n",
        "# =========================================================\n",
        "print(\"\\n--- Memulai Training Model 2: LSTM Summarizer ---\")\n",
        "\n",
        "df['headline_clean'] = df['headline'].apply(lambda x: 'sostok ' + str(x) + ' eostok')\n",
        "max_len_text = 60\n",
        "max_len_summary = 15\n",
        "\n",
        "# Tokenizer Summarizer\n",
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(df['short_description']))\n",
        "x_tr = pad_sequences(x_tokenizer.texts_to_sequences(df['short_description']), maxlen=max_len_text, padding='post')\n",
        "x_voc_size = len(x_tokenizer.word_index) + 1\n",
        "\n",
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(df['headline_clean']))\n",
        "y_tr = pad_sequences(y_tokenizer.texts_to_sequences(df['headline_clean']), maxlen=max_len_summary, padding='post')\n",
        "y_voc_size = len(y_tokenizer.word_index) + 1\n",
        "\n",
        "# Arsitektur Seq2Seq\n",
        "latent_dim = 256\n",
        "embedding_dim = 128\n",
        "\n",
        "encoder_inputs = Input(shape=(max_len_text,))\n",
        "enc_emb = Embedding(x_voc_size, embedding_dim, trainable=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True, return_sequences=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(y_voc_size, embedding_dim, trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model_summ = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model_summ.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "model_summ.fit([x_tr, y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:,1:],\n",
        "               epochs=5, batch_size=64, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrUD3TDFCNuR",
        "outputId": "10902e6f-22dc-486f-ef74-ffb30377b410"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Memulai Training Model 2: LSTM Summarizer ---\n",
            "Epoch 1/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 50ms/step - loss: 6.6742 - val_loss: 5.5057\n",
            "Epoch 2/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 48ms/step - loss: 5.3946 - val_loss: 5.4642\n",
            "Epoch 3/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 48ms/step - loss: 5.3732 - val_loss: 5.4280\n",
            "Epoch 4/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 47ms/step - loss: 5.2871 - val_loss: 5.3669\n",
            "Epoch 5/5\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 47ms/step - loss: 5.2134 - val_loss: 5.3212\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b55558a9a60>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4. EXPORT & FIX INFERENCE\n",
        "# =========================================================\n",
        "# Fix koneksi input-output agar tidak ValueError\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Input hanya yang terhubung ke Output\n",
        "decoder_model = Model(\n",
        "    inputs=[decoder_inputs, decoder_state_input_h, decoder_state_input_c],\n",
        "    outputs=[decoder_outputs2, state_h2, state_c2]\n",
        ")\n",
        "\n",
        "encoder_model.save('encoder_model.h5')\n",
        "decoder_model.save('decoder_model.h5')\n",
        "with open('x_tokenizer.pkl', 'wb') as f: pickle.dump(x_tokenizer, f)\n",
        "with open('y_tokenizer.pkl', 'wb') as f: pickle.dump(y_tokenizer, f)\n",
        "\n",
        "print(\"\\n[SELESAI] Semua file model siap dikirim ke Backend Flask!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzFik7f7CWGG",
        "outputId": "8e8d116e-54a1-4adf-a270-452ad6c8b731"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[SELESAI] Semua file model siap dikirim ke Backend Flask!\n"
          ]
        }
      ]
    }
  ]
}